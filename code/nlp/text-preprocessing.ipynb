{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import re\n",
    "import inflect\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_number_to_text(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    str_arr = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            word = p.number_to_words(word)\n",
    "        str_arr.append(word)\n",
    "    return ' '.join(str_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three balls in this bag, and twelve in the other one.'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'There are 3 balls in this bag, and 12 in the other one.'\n",
    "convert_number_to_text(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    res = text.translate(translator)\n",
    "    return res.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey did you know that the summer break is coming Amazing right  Its only 5 more days'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'Hey, did you know that the summer break is coming? Amazing right !! It\\'s only 5 more days !!'\n",
    "remove_punctuation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    return ' '.join(text.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We don't need the given questions\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = '   We don\\'t need   the given questions'\n",
    "remove_whitespace(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    res = [word for word in word_tokens if word not in stop_words]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan', 'Walker', 'sample', 'sentence', 'going', 'remove', 'stopwords']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'Alan Walker is a sample sentence and we are going to remove the stopwords from this.'\n",
    "remove_stopwords(remove_punctuation(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'Data science uses scientific methods algorithms and many types of processes'\n",
    "stem_words(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text): \n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'use',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'many',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "lemmatize_words(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('uses', 'VBZ'),\n",
       " ('scientific', 'JJ'),\n",
       " ('methods', 'NNS'),\n",
       " ('algorithms', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('many', 'JJ'),\n",
       " ('types', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('processes', 'NNS')]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "pos_tagging(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john has some cats\n",
      "cats eat fishs\n",
      "i eat a big fish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['john', 'cat'], ['cat', 'eat', 'fish'], ['eat', 'big', 'fish']]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'John has some cats', \n",
    "    'Cats eat fishs', \n",
    "    'I eat a big fish'\n",
    "]\n",
    "normalization = []\n",
    "\n",
    "for document in corpus:\n",
    "    document = document.lower()\n",
    "    print(document)\n",
    "    words = remove_stopwords(document)\n",
    "    words = lemmatize_words(' '.join(words))\n",
    "    \n",
    "    normalization.append(words)\n",
    "normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'cat', 'eat', 'fish', 'john']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for i in normalization:\n",
    "    for j in i:\n",
    "        if j not in words:\n",
    "            words.append(j)\n",
    "words.sort()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5        0.         0.         0.5       ]\n",
      " [0.         0.33333333 0.33333333 0.33333333 0.        ]\n",
      " [0.33333333 0.         0.33333333 0.33333333 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    te = [0] * len(normalization)\n",
    "    for j in range(len(normalization)):\n",
    "        if words[i] in normalization[j]:\n",
    "            te[j] += normalization[j].count(words[i])\n",
    "        te[j] /= len(normalization[j])\n",
    "    tf.append(te)\n",
    "    \n",
    "tf = np.array(tf).T\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2, 1]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [0] * len(words)\n",
    "\n",
    "for i in range(len(words)):\n",
    "    for document in normalization:\n",
    "        if words[i] in document:\n",
    "            sentences[i] += 1\n",
    "            \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6931471805599454,\n",
       " 1.2876820724517808,\n",
       " 1.2876820724517808,\n",
       " 1.2876820724517808,\n",
       " 1.6931471805599454]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = [0] * len(words)\n",
    "for i in range(len(words)):\n",
    "    idf[i] = math.log((len(normalization) + 1) / (sentences[i] + 1)) + 1\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.6438410362258904, 0.0, 0.0, 0.8465735902799727],\n",
       " [0.0, 0.42922735748392693, 0.42922735748392693, 0.42922735748392693, 0.0],\n",
       " [0.5643823935199818, 0.0, 0.42922735748392693, 0.42922735748392693, 0.0]]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = []\n",
    "\n",
    "for i in range(len(tf)):\n",
    "    te = []\n",
    "    for j in range(len(idf)):\n",
    "        te.append(tf[i][j] * idf[j])\n",
    "    tf_idf.append(te)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'cat', 'eat', 'fish', 'john']\n",
      "[[0.0, 0.6053485081062916, 0.0, 0.0, 0.7959605415681652], [0.0, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 0.0], [0.680918560398684, 0.0, 0.5178561161676974, 0.5178561161676974, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tf_idf)):\n",
    "    te = []\n",
    "    des = 0\n",
    "    for j in range(len(tf_idf[i])):\n",
    "        des += tf_idf[i][j] ** 2\n",
    "    for j in range(len(tf_idf[i])):\n",
    "        tf_idf[i][j] /= math.sqrt(des)\n",
    "print(words)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big' 'cats' 'eat' 'fish' 'fishs' 'john']\n",
      "  (0, 1)\t0.6053485081062916\n",
      "  (0, 5)\t0.7959605415681652\n",
      "  (1, 4)\t0.680918560398684\n",
      "  (1, 2)\t0.5178561161676974\n",
      "  (1, 1)\t0.5178561161676974\n",
      "  (2, 3)\t0.6227660078332259\n",
      "  (2, 0)\t0.6227660078332259\n",
      "  (2, 2)\t0.4736296010332684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "x = tfidfvectorizer.fit_transform(corpus)\n",
    "tfidfvectorizer.get_feature_names_out()\n",
    "print(tfidfvectorizer.get_feature_names_out())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdb7246361725cbabbd400ec1f68866d912854140cc201dca798721dd8db517b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
